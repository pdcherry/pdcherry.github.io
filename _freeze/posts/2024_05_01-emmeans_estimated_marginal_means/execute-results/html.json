{
  "hash": "97df1f7367e2a3104982fa6b6b8b9b16",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"emmeans: estimated marginal means\"\nauthor: \"Patrick Cherry\"\ndate: \"xxxx-xx-xx\"\ncategories:\n  - data\n  - code\n  - experimental analysis\n  - unbalanced DoE\n#image: \"2024_02_10-SF_tree_class_data_data/\"\n# PATRICK TO DO: FIND UNBALANCED THEME IMAGE\nformat:\n  html:\n    df-print: kable\nexecute:\n  freeze: TRUE\n  echo: TRUE\neditor_options: \n  chunk_output_type: inline\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# Introduction to marginal means\nEstimated marginal means (EMMs, previously known as least-squares means in the context of traditional regression models) are derived by using a model to make predictions over a regular grid of predictor combinations (called a reference grid).\n\nThere are many advantages to using estimated marginal means to arrive at effect sizes (as well as confidence intervals) for a multi-factor / multi-level experiment.\n\n  1. One challenge where EMMs really shines is when the number of subjects / samples per condition are not all the same or close to the same; this situation is called an \"unbalanced experiment,\" and it occurs often during large Design of Experiment (DoE), either intentionally by design, or unintentionally due to experimental sample drop outs. EMMs can provide accurate estimates of means and standard errors (confidence intervals) for conditions that differ in sample size, such that, all else equal, conditions with a smaller _n_ have larger confidence intervals.\n  1. Another is when there is a hypothesized (or evident) interaction effect in either the mean or the spread of one or more conditions in the experiment. EMMs can estimate accurate relative effects and standard errors (confidence intervals) for each of those interactions.\n\n# The pigs dataset\nConsider the pigs dataset provided with the package (`help(\"pigs\")` provides details). These data come from an experiment where pigs are given different percentages of protein (percent) from different sources (source) in their diet, and later we measured the concentration (conc) of the amino acid leucine. The percent values are quantitative, but we chose those particular values deliberately (like a DoE), and (at least initially) we want separate estimates at each percent level; that is, we want to view percent as a factor, not a quantitative predictor.\n\n## Initial model fitting\nOur first task is to come up with a good model. Doing so requires a lot of skill, and we don’t want to labor too much over the details; you really need other references to deal with this aspect adequately. But we will briefly discuss five models and settle on one of them:\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(conc ~ source * factor(percent), data = pigs)\npar(mfrow = c(2,2)); plot(mod1)\n```\n\n::: {.cell-output-display}\n![](2024_05_01-emmeans_estimated_marginal_means_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmod2 <- update(mod1, . ~ source + factor(percent))   # no interaction\npar(mfrow = c(2,2)); plot(mod2)\n```\n\n::: {.cell-output-display}\n![](2024_05_01-emmeans_estimated_marginal_means_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_dfr(list(mod1, mod2), glance)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| r.squared| adj.r.squared|    sigma| statistic|   p.value| df|    logLik|      AIC|      BIC| deviance| df.residual| nobs|\n|---------:|-------------:|--------:|---------:|---------:|--:|---------:|--------:|--------:|--------:|-----------:|----:|\n| 0.8083816|     0.6843933| 4.716024|  6.519819| 0.0003417| 11| -78.38304| 182.7661| 200.5409| 378.0950|          17|   29|\n| 0.6996728|     0.6343843| 5.075926| 10.716628| 0.0000207|  5| -84.89886| 183.7977| 193.3688| 592.5957|          23|   29|\n\n</div>\n:::\n:::\n\n\n\nThese models have R2 values of 0.808 and 0.700, and adjusted R2 values of 0.684 and 0.634. mod1 is preferable to mod2, suggesting we need the interaction term. However, a residual-vs-predicted plot of mod2 has a classic “horn” shape (curving and fanning out), indicating a situation where a response transformation might help better than including the interaction.\n\nAfter trial and error, it turns out that an inverse (reciprocal) transformation, (1/conc) really serves well. (Perhaps this isn’t too surprising, as concentrations are typically determined by titration, in which the actual measurements are volumes of some counter-reactant; and these are reciprocally related to concentrations, i.e., amounts per unit volume.)\n\nSo here are three more models:\n\n::: {.cell}\n\n```{.r .cell-code}\nmod3 <- update(mod1, inverse(conc) ~ .)\nmod4 <- update(mod2, inverse(conc) ~ .)     # no interaction\nmod5 <- update(mod4, . ~ source + percent)  # linear term for percent\npar(mfrow = c(2,2)); plot(mod5)\n```\n\n::: {.cell-output-display}\n![](2024_05_01-emmeans_estimated_marginal_means_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_dfr(list(mod1, mod2, mod3, mod4, mod5), glance)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| r.squared| adj.r.squared|     sigma| statistic|   p.value| df|    logLik|       AIC|       BIC|    deviance| df.residual| nobs|\n|---------:|-------------:|---------:|---------:|---------:|--:|---------:|---------:|---------:|-----------:|-----------:|----:|\n| 0.8083816|     0.6843933| 4.7160240|  6.519819| 0.0003417| 11| -78.38304|  182.7661|  200.5409| 378.0950000|          17|   29|\n| 0.6996728|     0.6343843| 5.0759265| 10.716628| 0.0000207|  5| -84.89886|  183.7977|  193.3688| 592.5956760|          23|   29|\n| 0.8175928|     0.6995647| 0.0031259|  6.927099| 0.0002349| 11| 133.86797| -241.7359| -223.9611|   0.0001661|          17|   29|\n| 0.7866455|     0.7402641| 0.0029065| 16.960361| 0.0000005|  5| 131.59563| -249.1912| -239.6202|   0.0001943|          23|   29|\n| 0.7487292|     0.7185767| 0.0030254| 24.831412| 0.0000001|  3| 129.22377| -248.4475| -241.6111|   0.0002288|          25|   29|\n\n</div>\n:::\n:::\n\n(Note: We could have used 1/conc as the response variable, but emmeans provides an equivalent inverse() function that will prove more advantageous later.) The residual plots for these models look a lot more like a random scatter of points (and that is good). The R2 values for these models are 0.818, 0.787, and 0.749, respectively; and the adjusted R2s are 0.700, 0.740, and 0.719. mod4 has the best adjusted R2 and will be our choice.\n\n# Estimated marginal means\n\n::: {.cell}\n\n```{.r .cell-code}\n(EMM.source <- emmeans(mod4, \"source\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n source emmean       SE df lower.CL upper.CL\n fish   0.0337 0.000926 23   0.0318   0.0356\n soy    0.0257 0.000945 23   0.0237   0.0276\n skim   0.0229 0.000994 23   0.0208   0.0249\n\nResults are averaged over the levels of: percent \nResults are given on the inverse (not the response) scale. \nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(EMM.percent <- emmeans(mod4, \"percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n percent emmean       SE df lower.CL upper.CL\n       9 0.0322 0.001032 23   0.0301   0.0344\n      12 0.0270 0.000969 23   0.0250   0.0290\n      15 0.0263 0.001104 23   0.0240   0.0286\n      18 0.0241 0.001337 23   0.0213   0.0268\n\nResults are averaged over the levels of: source \nResults are given on the inverse (not the response) scale. \nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\nCalling `tidy()` (from `broom`) on the object will put it into a beautiful data frame. we could make a plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(EMM.percent)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| percent|  estimate| std.error| df| statistic| p.value|\n|-------:|---------:|---------:|--:|---------:|-------:|\n|       9| 0.0322476| 0.0010322| 23|  31.24043|       0|\n|      12| 0.0270034| 0.0009688| 23|  27.87243|       0|\n|      15| 0.0262765| 0.0011040| 23|  23.80154|       0|\n|      18| 0.0240784| 0.0013370| 23|  18.00911|       0|\n\n</div>\n:::\n:::\n\n\n\n## Comparison to ordinary means\nLet’s compare these with the ordinary marginal means (OMMs) on inverse(conc):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(pigs, tapply(inverse(conc), source, mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      fish        soy       skim \n0.03331687 0.02632333 0.02372024 \n```\n\n\n:::\n:::\n\n\nCan I write the above ordinary means in Tidyverse/dplyr language? \n\n::: {.cell}\n\n```{.r .cell-code}\npigs %>%\n  mutate(conc = 1/conc) %>%\n  summarize(mean = mean(conc), .by = source)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|source |      mean|\n|:------|---------:|\n|fish   | 0.0333169|\n|soy    | 0.0263233|\n|skim   | 0.0237202|\n\n</div>\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(pigs, tapply(inverse(conc), percent, mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         9         12         15         18 \n0.03146170 0.02700341 0.02602757 0.02659336 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npigs %>%\n  mutate(conc = 1/conc) %>%\n  summarize(mean = mean(conc), .by = percent)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| percent|      mean|\n|-------:|---------:|\n|       9| 0.0314617|\n|      12| 0.0270034|\n|      15| 0.0260276|\n|      18| 0.0265934|\n\n</div>\n:::\n:::\n\n\nBoth sets of OMMs are vaguely similar to the corresponding EMMs. However, please note that the EMMs for percent form a decreasing sequence, while the the OMMs decrease but then increase at the end.\n\n# The reference grid, and definition of EMMs\n\nEstimated marginal means are defined as marginal means of model predictions over the grid comprising all factor combinations – called the reference grid. For the example at hand, the reference grid is\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(RG <- expand.grid(source = levels(pigs$source), percent = unique(pigs$percent)))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|source | percent|\n|:------|-------:|\n|fish   |       9|\n|soy    |       9|\n|skim   |       9|\n|fish   |      12|\n|soy    |      12|\n|skim   |      12|\n|fish   |      15|\n|soy    |      15|\n|skim   |      15|\n|fish   |      18|\n|soy    |      18|\n|skim   |      18|\n\n</div>\n:::\n:::\n\n\nTo get the EMMs, we first need to obtain predictions on this grid:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(preds <- matrix(predict(mod4, newdata = RG), nrow = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]       [,2]       [,3]       [,4]\n[1,] 0.03853514 0.03329091 0.03256404 0.03036586\n[2,] 0.03050486 0.02526063 0.02453376 0.02233558\n[3,] 0.02770292 0.02245869 0.02173182 0.01953364\n```\n\n\n:::\n:::\n\n\nthen obtain the marginal means of these predictions:\n\n::: {.cell}\n\n```{.r .cell-code}\napply(preds, 1, mean)   # row means -- for source\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03368899 0.02565870 0.02285677\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\napply(preds, 2, mean)   # column means -- for percent\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03224764 0.02700341 0.02627654 0.02407836\n```\n\n\n:::\n:::\n\n\nThese marginal averages match the EMMs obtained earlier via `emmeans()`.\n\nNow let’s go back to the comparison with the ordinary marginal means. The source levels are represented by the columns of pred; and note that each row of pred is a decreasing set of values. So it is no wonder that the marginal means – the EMMs for source – are decreasing. That the OMMs for percent do not behave this way is due to the imbalance in sample sizes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(pigs, table(source, percent))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      percent\nsource 9 12 15 18\n  fish 2  3  2  3\n  soy  3  3  3  1\n  skim 3  3  2  1\n```\n\n\n:::\n:::\n\n\nThis shows that the OMMs of the last column give most of the weight (3/5) to the first source, which tends to have higher inverse(conc), making the OMM for 18 percent higher than that for 15 percent, even though the reverse is true with every level of source. This kind of disconnect is an example of Simpson’s paradox, in which a confounding factor can distort your findings. The EMMs are not subject to this paradox, but the OMMs are, when the sample sizes are correlated with the expected values.\n\nIn summary, we obtain a references grid of all factor combinations, obtain model predictions on that grid, and then the expected marginal means are estimated as equally-weighted marginal averages of those predictions. Those EMMs are not subject to confounding by other factors, such as might happen with ordinary marginal means of the data. Moreover, unlike OMMs, EMMs are based on a model that is fitted to the data.\n\n",
    "supporting": [
      "2024_05_01-emmeans_estimated_marginal_means_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}