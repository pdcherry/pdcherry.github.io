{
  "hash": "600a58593042a1da802436462354255e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Copy-number-variant (CNV) TE Panel Analysis\"\nauthor: \"Patrick Cherry\"\ndate: \"2024-07-15\"\ncategories:\n  - data\n  - code\n  - target enrichment\n  - CNV\n  - next generation sequencing\n  - genetics\n  - genomics\nimage: \"2024_07_15-CNV_TE_panel_analysis/2024_07_15-CNV_TE_cnv_level_dist.png\"\nformat:\n  html:\n    df-print: kable\nexecute:\n  freeze: TRUE\n  echo: TRUE\neditor_options: \n  chunk_output_type: inline\n---\n\n\n\n# Summary\nIn this hypothetical exercise, a recently characterized disorder called under study is “CNV-emia”, an genetic disease showing autosomal recessive inheritance of mutations in the “CNSL” gene. The deletion/duplication breakpoints can vary from sample to sample (see table below), and it has been hypothesized that the breakpoints correspond with ethnicity. The breakpoint positions are known from the literature.\n\n| Del/Dup index | 5´ breakpoint | 3´ breakpoint |\n|---------------|---------------|---------------|\n| 1 | CNSL_probe_32 | CNSL_probe_38 |\n| 2 | CNSL_probe_27 | CNSL_probe_34 |\n| 3 | CNSL_probe_20 | CNSL_probe_40 |\n| 4 | CNSL_probe_10 | CNSL_probe_40 |\n\nThree probes with off-trend uniformity are CNSL_5, CNSL_23, and CNSL_46. I would recommend these probes be re-designed for better uniformity of capture and lower off-target.\n\nThe CNV calling and statistical analysis indicated that, significantly, the duplication vs. deletion of a CNV helps predict ethnicity. While the hypothesized probe intervals were not statistically significant in the model, the control data show that the method has high precision. In the CNSL region, ethnicity A has a CNV frequency of 3.2%, followed by B with a frequency of 1.0%, followed lastly by C with a frequency of 0.8%; in terms of CNV type, A is 1.8x more likely to have a deletion than a duplication, B is heavily biased toward deletions, and C is exclusively deletions (n = 21). Due to interval overlap of many hypothesized breakpoints, the model is partly confounded because the probe CNV calls are not fully independent.\n\nIn order to predict the unknown ethnicity of another set of data, I would train a classifier machine-learning model, such as a random forest classifier, a logistic model, or a K-nearest neighbors model, on the data in this analysis. Best practice is to separate training, validation, and test data. Once trained, parameters optimized, and predictions reasonably accurate, I would apply the model to the unknown data.\n\n# Procedure\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse, quietly = TRUE)\nlibrary(rlang)\nlibrary(here)\nlibrary(magrittr)\nlibrary(patchwork)\nlibrary(valr)\nlibrary(flextable)\nlibrary(mclogit)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_csv(data_dir, show_col_types = FALSE) %>%\n  dplyr::rename(\"id\" = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNew names:\n• `` -> `...1`\n```\n\n\n:::\n:::\n\n\n\n## Exploratory & Checks\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(data) %>% str_detect(\"probe\") %>% sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 100\n```\n\n\n:::\n:::\n\n\n\n\n\n### How many are CNSL vs. non-CNSL\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(data) %>%\n  as_tibble() %>%\n  filter(str_detect(value, \"probe\")) %>%\n  mutate(\"probe_type\" = case_when(str_detect(value, \"non_CNSL_probe_\") ~ \"non-CNSL\",\n                                  str_detect(value, \"CNSL_probe_\") ~ \"CNSL\",\n                                  TRUE ~ NA)) %>%\n  count(probe_type)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|probe_type |  n|\n|:----------|--:|\n|CNSL       | 50|\n|non-CNSL   | 50|\n\n</div>\n:::\n:::\n\n\n\n### Check id uniqueness\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount(data);\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|     n|\n|-----:|\n| 10000|\n\n</div>\n:::\n\n```{.r .cell-code}\ncount(count(data, id, name = \"num_occurences\"))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|     n|\n|-----:|\n| 10000|\n\n</div>\n:::\n:::\n\n\n\nAll ids are unique.\n\n\n\n\n\n\n\nEthnicities are not balanced in the experiment.\n\n### Distributions of probe counts\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobe_count_hist_facet <- data %>%\n  pivot_longer(contains(\"CNSL_probe\"), values_to = \"counts\", names_to = \"probe\") %>%\n  ggplot(aes(counts, fill = ethnicity)) +\n  geom_histogram() +\n  facet_wrap(~ probe)\nprobe_count_hist_facet + theme(strip.text = element_blank(), axis.text = element_text(size = 5))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](2024_07_15-CNV_TE_panel_analysis_files/figure-html/Distributions of probe counts-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggsave(paste0(\"file_prefix\", \"_\", \"probe_count_hist_facet\", \".png\"), probe_count_hist_facet,\n       width = 10, height = 10, dpi = 320)\n```\n:::\n\n\n\n# Normalize probe coverage\n### Normalize probe coverage withing samples (first)\nSo there are 100 probe columns. The only other columns are `id` and `ethnicity`. We have 50 CNSL probes (which are hypothesized to contribute to \"CNV-emia\", an autosomal recessive disease), and 50 non-CNSL probes in the target enrichment capture.\n\n\"Due to variability of extraction efficiency in the lab and error in the quantification of DNA libraries, each sample has a slightly different average NGS read depth across all probes.\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm_data <- data %>%\n  rowwise() %>%\n  # compute mean coverage for each sample (rowwise)\n  mutate(\"mean_sample_cov\" = mean(c_across(contains(\"_probe_\"))),\n         \"n_probes_in_mean\" = length(c_across(contains(\"_probe_\")))) %>%\n  ungroup() %>%\n  relocate(mean_sample_cov, n_probes_in_mean, .after = ethnicity) %>%\n  # compute sample-normalized coverage for each probe\n  mutate(across(.cols = contains(\"_probe_\"),\n                .fns = c(\"norm_cov\" = ~ .x / mean_sample_cov),\n                .names = \"{.fn}_{.col}\"),\n         .by = id)\n```\n:::\n\n\n\n### Normalize probe coverage per probe across samples (second)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm_data_filtered <- norm_data %>%\n  select(!c(\"CNSL_probe_5\", \"CNSL_probe_23\", \"CNSL_probe_46\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm_norm_data <- norm_data_filtered %>%\n  mutate(across(.cols = contains(\"norm_cov\"),\n                .fns = c(\"norm\" = ~ .x / mean(.x) #,\n                         # n_samples_in_norm originally here to ensure 10,000 samples per probe were in calc.\n                         #\"n_samples_in_norm\" = ~ length(.x)\n                         ),\n                .names = \"{.fn}_{.col}\"))\n```\n:::\n\n\n\nNormalize to coverage levels empirically seen in nonCNSL probes based on the assumptions of “nonCNSL” regions are CN=2 and ~1 or ~3, respectively. Thus, we can use the coverage data from nonCNSL probes to \"calibrate\" what a CN=2 sample looks like in this assay, after normalizing these data by sample and probe in the same way we did for the CNSL data.\n\n## Pivot 2x normalized data to long format\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm_norm_data_long <- norm_norm_data %>%\n  select(id, ethnicity, contains(\"norm_norm_cov\")) %>%\n  pivot_longer(contains(\"norm_norm_cov\"), values_to = \"norm_norm_cov\", names_to = \"probe\") %>%\n  # create order for probes\n  mutate(\"probe_number\" = as.integer(str_extract(probe, \"(?<=CNSL_probe_)\\\\d{1,2}\")),\n         \"probe_ordering\" = if_else(str_detect(probe, \"non_\"),\n                                  probe_number + 50, probe_number)) %>%\n  # simplify names for easier plotting\n  mutate(\"probe\" = str_remove(probe, \"norm_norm_cov_\"),\n         \"probe\" = str_remove(probe, \"probe_\"),\n         # impose integer ordering on probe name strings\n         \"probe\" = fct_reorder(as_factor(probe), probe_ordering)) %>%\n  select(!probe_ordering)\n```\n:::\n\n\n\n#### Save 2x normalized data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(norm_norm_data_long, paste0(file_prefix, \"_norm_norm_data_long\", \".csv.gz\"))\n```\n:::\n\n\n\n#### quick plot to check results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm_norm_count_hist_facet <- norm_norm_data_long %>%\n  ggplot(aes(norm_norm_cov, fill = ethnicity)) +\n  geom_histogram(binwidth = .1) +\n  scale_y_continuous(transform = \"log1p\") +\n  coord_cartesian(xlim = c(0, 2.5)) +\n  facet_wrap(~ probe + ethnicity) +\n  theme(strip.text = element_text(size = 3, margin = margin(0, 0, 0, 0)),\n        axis.text = element_text(size = 4))\n\nnorm_norm_count_hist_facet + theme(strip.text = element_blank(), axis.text = element_text(size = 4))\n```\n\n::: {.cell-output-display}\n![](2024_07_15-CNV_TE_panel_analysis_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggsave(paste0(file_prefix, \"_\", \"norm_norm_count_hist_facet\", \".png\"), norm_norm_count_hist_facet,\n       width = 10, height = 10, dpi = 320)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncopy_n_res <- 0.17\n\nCNSL_30_norm_norm_hist <- norm_norm_data_long %>%\n  filter(probe == \"CNSL_30\") %>%\n  ggplot(aes(norm_norm_cov, fill = ethnicity)) +\n  geom_histogram(binwidth = .01) +\n  geom_vline(xintercept = 1 - copy_n_res, linewidth = .25, linetype = \"dashed\") +\n  geom_vline(xintercept = 1 + copy_n_res, linewidth = .25, linetype = \"dashed\") +\n  scale_y_continuous(transform = \"log1p\") +\n  #coord_cartesian(xlim = c(0, 2.5)) +\n  facet_wrap(~ probe + ethnicity)\nCNSL_30_norm_norm_hist\n```\n\n::: {.cell-output-display}\n![](2024_07_15-CNV_TE_panel_analysis_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggsave(paste0(file_prefix, \"_\", \"CNSL_30_norm_norm_hist\", \".png\"), CNSL_30_norm_norm_hist,\n       width = 5, height = 3, dpi = 320)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nnorm_norm_cov_lineplot <- norm_norm_data_long %>%\n  select(id, ethnicity) %>%\n  slice_sample(n = 33, by = ethnicity) %>%\n  left_join(norm_norm_data_long, by = c(\"id\", \"ethnicity\")) %>%\n  ggplot(aes(y = norm_norm_cov, x = probe_number, color = ethnicity)) +\n  geom_line() +\n  facet_wrap(~ id, ncol = 1) +\n  theme(strip.text = element_text(size = 3, margin = margin(0, 0, 0, 0)),\n        axis.text = element_text(size = 4))\n\nggsave(paste0(file_prefix, \"_\", \"norm_norm_cov_lineplot\", \".png\"), norm_norm_cov_lineplot,\n       width = 5, height = 20, dpi = 320)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprobe_norm_count_hist_facet <- norm_data %>%\n  pivot_longer(contains(\"norm_cov\"), values_to = \"norm_counts\", names_to = \"probe\") %>%\n  ggplot(aes(norm_counts, fill = ethnicity)) +\n  geom_histogram(binwidth = 0.1) +\n  facet_wrap(~ probe)\n\nprobe_norm_count_hist_limited_facet <- norm_data %>%\n  pivot_longer(contains(\"norm_cov\"), values_to = \"norm_counts\", names_to = \"probe\") %>%\n  mutate(\"probe\" = str_remove(probe, \"norm_cov_\")) %>%\n  arrange(probe) %>%\n  filter(probe %in% c(\"CNSL_probe_5\", \"CNSL_probe_23\", \"CNSL_probe_46\",\n                      \"CNSL_probe_1\", \"CNSL_probe_2\", \"non_CNSL_probe_2\")) %>%\n  ggplot(aes(norm_counts, fill = ethnicity)) +\n  geom_histogram(binwidth = 0.1) +\n  facet_wrap(~ probe) +\n  labs(title = \"Normalized count histograms\",\n       subtitle = \"Example set, for demonstration\",\n       y = \"Sample count (n)\",\n       x = \"Normalized counts (ratio to mean)\")\n\nprobe_norm_count_hist_limited_facet\n```\n\n::: {.cell-output-display}\n![](2024_07_15-CNV_TE_panel_analysis_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggsave(paste0(\"file_prefix\", \"_\", \"probe_norm_count_hist_facet\", \".png\"), probe_norm_count_hist_facet,\n       width = 10, height = 10, dpi = 320)\n```\n:::\n\n\n\nMost of the normalized coverage distributions of probes look great! The per-sample normalization worked well. However, we can see (highlighted specifically here, in the example) some probes that are not behaving well.\n\n### Identify bad probes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobe_eval_df <- norm_norm_data_long %>%\n  summarize(across(norm_norm_cov,\n                   .fns = c(\"mean\" = mean,\n                            \"sd\" = sd,\n                            \"min\" = min\n                            #\"max\" = max\n                            ),\n                   .names = \"{.fn}_{.col}\"),\n            #\"n_samples\" = n(),\n            .by = c(probe, probe_number)) %>%\n  mutate(across(where(is.numeric), ~ signif(.x, 3)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprobe_uniformity_dist_plot <- probe_eval_df %>%\n  ggplot(aes(sd_norm_norm_cov)) +\n  geom_histogram(binwidth = 0.025) +\n  labs(title = \"Probe uniformity check\",\n       subtitle = \"Distribution of standard deviations of coverages by probe\",\n       y = \"number of probes\",\n       x = \"Stadard deviation of coverage\")\nprobe_uniformity_dist_plot\n```\n\n::: {.cell-output-display}\n![](2024_07_15-CNV_TE_panel_analysis_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\nSo we have three outlier probes with poor uniformity of coverage. Let's list out what they are.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobe_eval_df %>% arrange(desc(sd_norm_norm_cov)) %>% slice_head(n = 6)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|probe       | probe_number| mean_norm_norm_cov| sd_norm_norm_cov| min_norm_norm_cov|\n|:-----------|------------:|------------------:|----------------:|-----------------:|\n|CNSL_5      |            5|                  1|            1.010|            0.0000|\n|CNSL_23     |           23|                  1|            0.512|            0.0362|\n|CNSL_46     |           46|                  1|            0.512|            0.0409|\n|CNSL_32     |           32|                  1|            0.111|            0.4350|\n|CNSL_36     |           36|                  1|            0.108|            0.4270|\n|non_CNSL_23 |           23|                  1|            0.100|            0.6320|\n\n</div>\n:::\n:::\n\n\n\nIn this head of the table of uniformity sorted by standard deviation of the probes, the three probes with off-trend uniformity are CNSL_5, CNSL_23, and CNSL_46. I would recommend these probes be re-designed for better uniformity of capture and lower off-target.\n\nIn target enrichment, probes that perform poorly (due to their design / positioning) typically exhibit high rates of off-target reads (not directly observable in this data set) and a high variability in coverage from sample-to-sample (*i.e.* low uniformity of coverage). This analysis uses a rough measure of uniformity to identify potentially poorly-performing probes.\n\n### Filter out bad probes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm_norm_data_long %<>% filter(str_detect(probe, \"(?<!non_)(CNSL_5|CNSL_23|CNSL_46)\", negate = TRUE))\n```\n:::\n\n\n\n## Convert coverage to integer CNV levels\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnonCNSL_CNV_long <- norm_norm_data_long %>%\n    # compute non-CNSL coverage means\n    filter(str_detect(probe, \"non\")) %>%\n    mutate(\"mean_nonCNSL_cov\" = mean(norm_norm_cov),\n           \"sd_nonCNSL_cov\" = sd(norm_norm_cov),\n           \"n_samples\" = n(),\n           .by = probe)\n\nnonCNSL_CNV_long %>%\n  distinct(probe, mean_nonCNSL_cov, n_samples) %>%\n  summarise(\"mean cov for all non-CNSL probes\" = mean(mean_nonCNSL_cov),\n            \"SD cov for all non-CNSL probes\" = sd(mean_nonCNSL_cov),\n            \"min cov\" = min(mean_nonCNSL_cov),\n            \"max cov\" = max(mean_nonCNSL_cov),\n            \"n probes\" = n(),\n            \"n samples in each mean\" = mean(n_samples))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| mean cov for all non-CNSL probes| SD cov for all non-CNSL probes| min cov| max cov| n probes| n samples in each mean|\n|--------------------------------:|------------------------------:|-------:|-------:|--------:|----------------------:|\n|                                1|                              0|       1|       1|       50|                  10000|\n\n</div>\n:::\n:::\n\n\n\nThis analysis shows that all non-CNSL probes have a mean coverage (across the 10,000 samples sequenced) of exactly 1.0 . Thus, to convert these coverage values to CNV values (normalized haploid genomic equivalents), we must multiply by 2. (The standard deviation is so small as to be negligible, and is likely caused by floating point calculation error accumulation. This is supported by the min and max calculations in the next columns of the summary table.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm_norm_data_long %<>% mutate(\"CNV_level\" = norm_norm_cov * 2)\n```\n:::\n\n\n\n#### Save 2x normalized data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(norm_norm_data_long, paste0(file_prefix, \"_norm_norm_data_long\", \".csv.gz\"))\n```\n:::\n\n\n\n## Analyze to detect copy number variations (unsing rle)\nRun length encoding is a lossless form of compression that encodes a data series as a different series where each consecutive repeated value is recorded as the value and the number of repeats. In R, this is accomplished with the `rle` function, and the output is a linked pair of lists where the values and number of repeats occupy the same position in their respective lists.\n\nWe can take advantage of run length encoding not for its compression properties, but for its ability to variably call the number of consecutive values. We will use `TRUE`/`FALSE` binary values (or `Dup`, `Del`, and `No CNV` values) for whether there is a CNV (deletion or duplication).\n\nHowever, because we have eliminated three probes from the analysis, and because the start and stop positions of the CNV will be reported in units of _*list elements*_ —not probes—, we have to create a secondary positional order column. This will allow for seamless consecutive CNV calling, and for conversion back to probe number annotations used in the original dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm_norm_data_long %<>%\n  # new code to correct for probe position\n  mutate(\"probe_CNSL_target\" = str_extract(probe, \"(non_)*CNSL\")) %>%\n  mutate(\"continuous_probe_id\" = row_number() - 1, # row_number() starts at 1\n         .by = c(id, ethnicity, probe_CNSL_target))\n```\n:::\n\n\n\n\nAs expected, there should be 50 non-CNSL probes, and 47 CNSL probes.\n\n### Run rle CNV detection\n#### Make plot to set threshold by eye\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncnv_level_dist_tbl <- norm_norm_data_long %>%\n  summarize(\"Mean CNV\" = mean(CNV_level),\n            \"Max CNV\" = max(CNV_level),\n            \"Min CNV\" = min(CNV_level),\n            .by = c(id, ethnicity)) %>%\n  # pivot longer for faceted plots\n  pivot_longer(cols = c(\"Mean CNV\", \"Max CNV\", \"Min CNV\"),\n               names_to = \"Measure\", values_to = \"value\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncnv_call_threshold <- 0.9\n\ncnv_level_dist <- cnv_level_dist_tbl %>%\n  ggplot(aes(value, fill = ethnicity)) +\n  geom_histogram(binwidth = .01) +\n  geom_vline(xintercept = 2, linetype = \"dotdash\", linewidth = .3) +\n  geom_vline(xintercept = 1.33, linetype = \"dashed\", linewidth = .3) +\n  geom_vline(xintercept = 2.75, linetype = \"dashed\", linewidth = .3) +\n  scale_y_continuous(transform = \"log10\") +\n  facet_wrap(~ Measure) +\n  # theme(strip.text = element_text(size = 3, margin = margin(0, 0, 0, 0)),\n  #       axis.text = element_text(size = 4)) +\nlabs(title = \"Distributions of CNV means, maxes, & mins by probe\",\n     subtitle = \"Dashed lines indicate CNV calling thresholds\",\n     x = \"CNV level\",\n     y = \"Count (number of samples) (log scale)\")\ncnv_level_dist\n```\n\n::: {.cell-output-display}\n![](2024_07_15-CNV_TE_panel_analysis_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\nThe above plot shows the distribution of CNV levels in samples, and puts the distribution in the context of manually (by eye) setting thresholds of a Deletion (low) or a Duplication (high).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggsave(paste0(file_prefix, \"_\", \"cnv_level_dist\", \".png\"), cnv_level_dist,\n       width = 8, height = 5, dpi = 320)\n```\n:::\n\n\n\n#### Apply thresholds in rle analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncnv_rle <- function(sub_df, lower_thresh, upper_thresh){\n  \n  # check that thresholds are valid\n  if(lower_thresh > upper_thresh) stop(\"Error: lower CNV threshold, `lower_thresh`, must be less than the upper CNV threshold, `upper_thresh`.\")\n  \n  sub_df_cnv_calls <- sub_df %>%\n    mutate(\"CNV_call\" = case_when(CNV_level <= lower_thresh ~ \"Deletion\",\n                                  CNV_level >= upper_thresh ~ \"Duplication\",\n                                  TRUE ~ \"No CNV\"))\n  \n  df_rle <- bind_cols(\n    \"rle_lengths\" = rle(sub_df_cnv_calls$CNV_call)[[1]],\n    \"rle_values\" = rle(sub_df_cnv_calls$CNV_call)[[2]]\n    ) %>%\n    mutate(\"start_pos\" = as.integer(lag(cumsum(rle_lengths))) + 1,\n           \"end_pos\" = as.integer(cumsum(rle_lengths))) %>%\n    replace_na(list(\"start_pos\" = 0))\n  \n  return(df_rle)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncnv_rle <- norm_norm_data_long %>%\n  nest(.by = c(\"id\", \"ethnicity\", \"probe_CNSL_target\")) %>%\n  mutate(\"rle_result\" = map(data, ~ cnv_rle(.x, lower_thresh = 1.33, upper_thresh = 2.75))) %>%\n  select(!data) %>%\n  unnest(c(rle_result)) %>%\n  mutate(\"CNV_call_length\" = as.integer(end_pos - start_pos))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncnv_rle_hits <- cnv_rle %>%\n  filter(rle_values != \"No CNV\" & CNV_call_length >= 4)\n```\n:::\n\n\n\n### Analyze the analysis\nIn the hits, are there any samples with more than one CNV call?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncnv_rle_hits %>% count(id, name = \"number CNV contigs\") %>% count(`number CNV contigs`, name = \"number of samples\")\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| number CNV contigs| number of samples|\n|------------------:|-----------------:|\n|                  1|               206|\n|                  2|                 8|\n|                  3|                 1|\n\n</div>\n:::\n:::\n\n\n\nIn this rle analysis using the above thresholds, 206 samples have one CNV detected, 8 samples have 2 CNVs called, and 1 sample has 3.\n\n\n\n\n\n\n\n\n\nHow many CNVs are too short to make the cut?\n\n\n\n\n\n\n1359 out of 21,951 total called CNVs have a consecutive length of less than 3 probes\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(cnv_rl_dist <- cnv_rle_hits %>%\n  ggplot(aes(CNV_call_length)) + geom_histogram(binwidth = 1) +\n  ggdist::stat_dist_pointinterval() +\n  labs(title = \"Distribution of called CNV run lengths\",\n       y = \"sample count\"))\n```\n\n::: {.cell-output-display}\n![](2024_07_15-CNV_TE_panel_analysis_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n\nThe above QC plot shows the distribution of the consecutive length of the called CNV regions. We see the median length is 6, with a standard deviation of 5.8; the distribution is skewed to the left, with some potential groupings of CNV run lengths.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncnv_rl_dist + facet_wrap(~ rle_values)\n```\n\n::: {.cell-output-display}\n![](2024_07_15-CNV_TE_panel_analysis_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\nThe deletions tend to occupy ~ 3 clusters fo lengths, around 6~7 probes, ~19 probes, and ~ 29 probes. The duplications are more clustered around ~5 probes in length.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncnv_rle_hits %>%\n  ggplot(aes(CNV_call_length)) + geom_histogram(binwidth = 1) +\n  facet_grid(ethnicity ~ rle_values, scales = \"free_y\") +\n  labs(title = \"Distribution of called CNV run lengths\",\n       subtitle = \"Faceted by ethnicity and CNV type\",\n       y = \"sample count\")\n```\n\n::: {.cell-output-display}\n![](2024_07_15-CNV_TE_panel_analysis_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncnv_rle_hits_tidy_bed <- cnv_rle %>%\n  # keep observations where No CNV is the call, or it's hit based on 4 consecutive probes\n  filter(rle_values == \"No CNV\" | CNV_call_length >= 4) %>%\n  # convert start positions to probe number\n  left_join(probe_number_lookup_tbl,\n            by = c(\"start_pos\" = \"continuous_probe_id\",\n                   \"probe_CNSL_target\"),\n            suffix = c(\"\", \"_start\")) %>%\n  # convert end positions to probe number\n  mutate(end_pos = end_pos - 1) %>%\n  left_join(probe_number_lookup_tbl,\n            by = c(\"end_pos\" = \"continuous_probe_id\",\n                   \"probe_CNSL_target\"),\n            suffix = c(\"_start\", \"_end\")) %>%\n  # prepare dataframe for bed tools (equivalent)\n  rename(\"chrom\" = probe_CNSL_target, \"start\" = probe_number_start, \"end\" = probe_number_end)\n```\n:::\n\n\n\nHere we have what looks like evidence we can classify the samples into ethnicity based on the status of the deletion v. duplication, and the run length of the consecutive number of probes affected by each type of genomic aberration.\n\n#### Hard-code hypothesized breakpoints\nNote that bed format specifies that the start position for an interval is zero based and the end position is one-based. So we will have to manually add 1 to the end position of the probes in the hypothesis table.\n\n\n\n\n\n\nUsing a R-dataframe-compatible version of [bedtools closest](https://bedtools.readthedocs.io/en/latest/content/tools/closest.html) to compute data for breakpoint hypotheses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# filtering out non-CNSL calls, because they are not in the hypothesis table\ncnv_rle_closest <- valr::bed_closest(filter(cnv_rle_hits_tidy_bed, chrom != \"non_CNSL\"),\n                                          breakpoints_bed_df,\n                                          overlap = TRUE) %>%\n  arrange(id.x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncnv_rle_hits_closest <- cnv_rle_closest %>%\n  # keep only CNV-positives\n  filter(rle_values.x != \"No CNV\") %>% \n  # tidy up factors and leveling\n  mutate(ethnicity.x = as_factor(ethnicity.x),\n         name.y = as_factor(name.y)) %>%\n  mutate(\"name.y\" = fct_relevel(name.y,\n                                \"CNSL_probe_32-CNSL_probe_38\", \"CNSL_probe_27-CNSL_probe_34\",\n                                \"CNSL_probe_20-CNSL_probe_40\", \"CNSL_probe_10-CNSL_probe_40\")) %>%\n  # sort for priority of keeping one per sample\n  arrange(id.x, .dist, desc(.overlap), name.y) %>%\n  # sort to prioritize lowest distance, highest overlap\n  distinct(id.x, .keep_all = TRUE)\n```\n:::\n\n::: {.cell layout-ncol=\"2\" tbl-subcap='[\"CNVs called per sample\",\"Unique samples with any CNVs\"]'}\n\n```{.r .cell-code}\ncnv_rle_closest %>%\n  count(id.x) %>%\n  summarize(\"mean CNVs per sample\" = mean(n),\n            \"SD CNVs per sample\" = sd(n))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| mean CNVs per sample| SD CNVs per sample|\n|--------------------:|------------------:|\n|                4.382|           1.907312|\n\n</div>\n:::\n\n```{.r .cell-code}\ncnv_rle_hits_closest %>% count()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|   n|\n|---:|\n| 215|\n\n</div>\n:::\n:::\n\n\n\n215 samples have a CNV called. 891 CNVs were called, meaning on average each positive sample has 4.14 CNVs called.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncnv_rle_summary <- cnv_rle_closest %>%\n    # tidy up factors and leveling\n  mutate(ethnicity.x = fct_relevel(as_factor(ethnicity.x), LETTERS[1:3]),\n         name.y = as_factor(name.y)) %>%\n  mutate(\"name.y\" = fct_relevel(name.y,\n                                \"CNSL_probe_32-CNSL_probe_38\", \"CNSL_probe_27-CNSL_probe_34\",\n                                \"CNSL_probe_20-CNSL_probe_40\", \"CNSL_probe_10-CNSL_probe_40\")) %>%\n  summarize(\"n\" = n(),\n            \"mean_CNV_length\" = mean(CNV_call_length.x),\n            \"sd_CNV_length\" = sd(CNV_call_length.x),\n            \"mean_overlap\" = mean(.overlap),\n            \"sd_overlap\" = sd(.overlap),\n            \"mean_distance\" = mean(abs(.dist)),\n            \"sd_distance\" = sd(abs(.dist)),\n    .by = c(ethnicity.x, rle_values.x, name.y)) %>%\n  mutate(across(where(is.double), ~ signif(.x, digits = 4))) %>%\n  mutate(\"percent\" = signif(100 * n / sum(n), digits = 3),\n         .by = c(ethnicity.x, name.y)) %>%\n  relocate(percent, .after = n) %>%\n  arrange(ethnicity.x, rle_values.x, name.y)\n```\n:::\n\n::: {.cell tbl-cap='CNV types and loci by ethnicity'}\n\n```{.r .cell-code}\ncnv_rle_summary %>%\n  rename(\"ethnicity\" = ethnicity.x, \"CNV type\" = rle_values.x, \"probe name\" = name.y) %>%\n  mutate(\"probe name\" = str_remove_all(`probe name`, \"CNSL_probe_\")) %>%\n  select(!c(sd_CNV_length, sd_overlap, sd_distance)) %>%\n  dplyr::rename(\"CNV_length\" = mean_CNV_length, \"overlap\" = mean_overlap, \"distance\" = mean_distance) %>%\n  filter(`CNV type` != \"No CNV\")\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|ethnicity |CNV type    |probe name |   n| percent| CNV_length| overlap| distance|\n|:---------|:-----------|:----------|---:|-------:|----------:|-------:|--------:|\n|A         |Deletion    |32-38      | 112|  2.0700|      6.491|   3.527|  0.00000|\n|A         |Deletion    |27-34      | 112|  2.0700|      6.491|   3.964|  0.00000|\n|A         |Deletion    |20-40      | 112|  2.0300|      6.491|   5.491|  0.00000|\n|A         |Deletion    |10-40      | 112|  2.0300|      6.491|   5.491|  0.00000|\n|A         |Duplication |32-38      |  62|  1.1500|      5.097|   2.629|  0.37100|\n|A         |Duplication |27-34      |  62|  1.1500|      5.097|   2.694|  0.08065|\n|A         |Duplication |20-40      |  62|  1.1300|      5.097|   4.097|  0.00000|\n|A         |Duplication |10-40      |  62|  1.1300|      5.097|   4.097|  0.00000|\n|B         |Deletion    |32-38      |  28|  1.0100|     17.250|   6.357|  0.14290|\n|B         |Deletion    |27-34      |  29|  1.0100|     16.830|   6.897|  0.06897|\n|B         |Deletion    |20-40      |  29|  0.9600|     16.830|  16.720|  0.00000|\n|B         |Deletion    |10-40      |  29|  0.9600|     16.830|  16.720|  0.00000|\n|B         |Duplication |32-38      |   1|  0.0359|      4.000|   2.000|  0.00000|\n|B         |Duplication |27-34      |   1|  0.0348|      4.000|   3.000|  0.00000|\n|B         |Duplication |20-40      |   1|  0.0331|      4.000|   3.000|  0.00000|\n|B         |Duplication |10-40      |   1|  0.0331|      4.000|   3.000|  0.00000|\n|C         |Deletion    |32-38      |  15|  0.5980|     19.800|   5.200|  0.26670|\n|C         |Deletion    |27-34      |  19|  0.7470|     17.680|   5.000|  1.05300|\n|C         |Deletion    |20-40      |  21|  0.8040|     16.670|  11.140|  0.38100|\n|C         |Deletion    |10-40      |  21|  0.7950|     16.670|  16.240|  0.00000|\n\n</div>\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(cnv_rle_summary, paste0(file_prefix, \"_\", \"cnv_rle_global_summary.csv\"))\n```\n:::\n\n\n\n#### Statistical analysis\nWe need to model the evidence in the data (CNVs, which locus they're in) to predict ethnicity. Ethnicity has 3 levels, CNVs have 3 (or 2 when omitting no CNV), and locus has 4 levels, so we need a multinomial logit models, which is what [`mclogit`](https://melff.github.io/mclogit/reference/mblogit.html) provides.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(probes_mblogit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nmblogit(formula = ethnicity.x ~ name.y + rle_values.x, data = ., \n    weights = n)\n\nEquation for B vs A:\n                                  Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                       -1.38520    0.20791  -6.662 2.69e-11 ***\nname.yCNSL_probe_27-CNSL_probe_34  0.03364    0.29053   0.116    0.908    \nname.yCNSL_probe_20-CNSL_probe_40  0.03364    0.29053   0.116    0.908    \nname.yCNSL_probe_10-CNSL_probe_40  0.03364    0.29053   0.116    0.908    \nrle_values.xDuplication           -2.76727    0.51474  -5.376 7.62e-08 ***\n\nEquation for C vs A:\n                                  Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                        -2.0102     0.2748  -7.314 2.59e-13 ***\nname.yCNSL_probe_27-CNSL_probe_34   0.2361     0.3701   0.638    0.524    \nname.yCNSL_probe_20-CNSL_probe_40   0.3362     0.3633   0.925    0.355    \nname.yCNSL_probe_10-CNSL_probe_40   0.3362     0.3633   0.925    0.355    \nrle_values.xDuplication           -16.8589   706.1172  -0.024    0.981    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate residual Deviance: 1076 \nNumber of Fisher scoring iterations:  14 \nNumber of observations:  891 \n```\n\n\n:::\n:::\n\n\n\n#### Interpretation of CNV data against hypothesized breakpoints\n*Ethnicity A* has CNVs in the CNSL region detected 3.2% of the time. When there is a CNV, 2.07% are Deletions in the either of the CNSL_probe_27-CNSL_probe_34 or CNSL_probe_32-CNSL_probe_38 regions, and 1.13% are Duplications, primarily in the CNSL_probe_20-CNSL_probe_40 region. Because CNSL_probe_10-CNSL_probe_40 is a superset of all other regions, we can conclude from the counts / frequencies being the same for all four hypothesized regions that these CNVs are occurring over the narrowest possible interpretation.\n\n*Ethnicity B* has CNVs in the CNSL region detected 1% of the time. When there is a CNV, 0.96% of the time it's a Deletion in CNSL_probe_20-CNSL_probe_40, and 0.0331% of the time (n = 1) it's a Duplication with overlap for both CNSL_probe_27-CNSL_probe_34\nand CNSL_probe_32-CNSL_probe_38.\n\n*Ethnicity C* has CNVs in the CNSL region detected 0.8% of the time. When there is a CNV, it's a Deletion in CNSL_probe_10-CNSL_probe_40 that occurs 0.795% of samples, with a mean overlap of 16.24 probes.\n\n### Global analysis of ethnicities\n\n\n::: {.cell tbl-cap='CNV types and counts by ethnicity'}\n\n```{.r .cell-code}\ncnv_rle_global_counts <- cnv_rle_hits_tidy_bed %>%\n  dplyr::rename(\"probe_CNSL_target\" = chrom) %>%\n  count(ethnicity, probe_CNSL_target, rle_values) %>%\n  mutate(\"percent\" = signif(n / sum(n) * 100, 3),\n         .by = c(ethnicity, probe_CNSL_target))\n\ncnv_rle_global_counts\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|ethnicity |probe_CNSL_target |rle_values  |    n|  percent|\n|:---------|:-----------------|:-----------|----:|--------:|\n|A         |CNSL              |Deletion    |  112|   2.0300|\n|A         |CNSL              |Duplication |   62|   1.1300|\n|A         |CNSL              |No CNV      | 5330|  96.8000|\n|A         |non_CNSL          |No CNV      | 4992| 100.0000|\n|B         |CNSL              |Deletion    |   29|   0.9600|\n|B         |CNSL              |Duplication |    1|   0.0331|\n|B         |CNSL              |No CNV      | 2991|  99.0000|\n|B         |non_CNSL          |No CNV      | 2551| 100.0000|\n|C         |CNSL              |Deletion    |   21|   0.7950|\n|C         |CNSL              |No CNV      | 2620|  99.2000|\n|C         |non_CNSL          |No CNV      | 2490| 100.0000|\n\n</div>\n:::\n:::\n\n\n\nThe above control table shows that, for all ethnicities, 100% of non-CNSL probes are **not** detected as having a CNV. Thus, based on these control data and our threshold settings, we can expect a false-positive rate (Type I error) very close to zero.\n\nIn the CNSL region, ethnicity A has a CNV frequency of 3.2%, followed by B with a frequency of 1.0%, followed lastly by C with a frequency of 0.8%; in terms of CNV type, A is 1.8x more likely to have a deletion than a duplication, B is heavily biased toward deletions, and C is exclusively deletions (n = 21).\n",
    "supporting": [
      "2024_07_15-CNV_TE_panel_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}