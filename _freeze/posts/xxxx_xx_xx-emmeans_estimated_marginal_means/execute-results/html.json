{
  "hash": "b69e9bd8f0944650c8d3c5e65b57ec7f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"emmeans: estimated marginal means\"\nauthor: \"Patrick Cherry\"\ndate: \"2024-05-19\"\ncategories:\n  - data\n  - code\n  - experimental analysis\n  - unbalanced DoE\nbibliography: 2024_05_19-emmeans_estimated_marginal_means.bib\nimage: \"2024_05_19-emmeans_estimated_marginal_means_pigs_cc.jpg\"\nformat:\n  html:\n    df-print: kable\nexecute:\n  freeze: TRUE\n  echo: TRUE\neditor_options: \n  chunk_output_type: inline\n---\n\n\n![The diet of pigs contributes to their rate of growth and macromolecular composition. Photo reproduced here from m_takahashi under CC BY-ND 2.0 Deed Attribution-NoDerivs 2.0 Generic](2024_05_19-emmeans_estimated_marginal_means_pigs_cc.jpg)\n\n\n\n::: {.cell}\n\n:::\n\n\n\n# Introduction to marginal means\nEstimated marginal means (EMMs, previously known as least-squares means in the context of traditional regression models) [@Searle1980] are derived by using a model to make predictions over a regular grid of predictor combinations (called a reference grid). Estimated marginal means have historically been used commonly in agricultural science publications.\n\n[Russ Lenth](https://stat.uiowa.edu/people/russell-v-lenth) authored the `emmeans` package (distributed [on CRAN](https://cran.r-project.org/web/packages/emmeans/)) to be an implementation of the \"least-squares\" means (which Lenth prefers to call \"marginal means\" for the reasons discussed below) in R.\n\nAs Searle, _et al._ write, there is a **marginal mean** for every variable (and every level of factor variables) in a data set, and a sufficiently defined model (be it a linear model (`lm`), `glm`, `lmer`, or `glmer` _etc_.) will allow for those marginal means to be estimated.\n\nBut I just used \"marginal\" in the definition of \"estimated marginal means.\" So what does marginal signify? To distill it down, a marginal mean is the effect size that a particular variable contributes, _all else being equal_. (The all else being equal part comes from accounting for all other _significant_ effects in the model.)\n\nThere are many advantages to using estimated marginal means to arrive at effect sizes (as well as confidence intervals) for a multi-factor / multi-level _experiment_.\n\n  1. One challenge where EMMs really shines is when the number of subjects / samples per condition are not all the same or close to the same; this situation is called an \"unbalanced experiment,\" and it occurs often during large Design of Experiment (DoE), either intentionally by design, or unintentionally due to experimental sample drop outs. EMMs can provide accurate estimates of means and standard errors (confidence intervals) for conditions that differ in sample size, such that, all else equal, conditions with a smaller _n_ have larger confidence intervals.\n  1. Another is when there is a hypothesized (or evident) interaction effect in either the mean or the spread of one or more conditions in the experiment. EMMs can estimate accurate relative effects and standard errors (confidence intervals) for each of those interactions.\n\n## Note on using EMMs for _experiments_\n::: {#EMMs-for-experiments .callout-tip}\n### EMMs are best used on data obtained from controlled experiments.\nEMMs are less applicable to observational data.\n:::\n\n__Add better explanation here__\n\n# The pigs dataset\nConsider the pigs dataset provided with the package (`help(\"pigs\")` provides details). These data come from an experiment where pigs are given different percentages of protein (percent) from different sources (source) in their diet, and later we measured the concentration (conc) of the amino acid leucine. The percent values are quantitative, but we chose those particular values deliberately (like a DoE), and (at least initially) we want separate estimates at each percent level; that is, we want to view percent as a factor, not a quantitative predictor.\n\n## Initial model fitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)          # dplyr, ggplot2, etc.\ntheme_set(theme_light())    # set default themes for ggplot in this doc to add contrast\nlibrary(broom)              # help tidy display and compare of model summary stats\nlibrary(emmeans)            # ~~The estimated marginal means package~~\nlibrary(patchwork)          # for combining plots\nlibrary(modelbased)         # https://easystats.github.io/modelbased/articles/estimate_means.html\n```\n:::\n\n\n\nOur first task is to come up with a good model. Making sure the model is appropriate for the data, experiment, and underlying scientific processes is critical to drawing valid conclusions of `emmeans`. Constructing good models is equal parts art and science, but I won't labor too much over the details; any reader is encouraged to seek more in-depth guidance on model construction and evaluation. I will construct and view some models and settle on one of them. The key model diagnostics I will keep an eye on are:\n\n - AIC\n - BIC\n - r-squared\n - Residuals vs Fitted (appropriate model)\n - Scale-Location ([homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity))\n - Residuals vs Leverage (outliers / overly-influential points)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(conc ~ source * factor(percent), data = pigs)\npar(mfrow = c(2,2)); plot(mod1)\n```\n\n::: {.cell-output-display}\n![](xxxx_xx_xx-emmeans_estimated_marginal_means_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmod2 <- update(mod1, . ~ source + factor(percent))   # no interaction\npar(mfrow = c(2,2)); plot(mod2)\n```\n\n::: {.cell-output-display}\n![](xxxx_xx_xx-emmeans_estimated_marginal_means_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_dfr(list(mod1, mod2), glance)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| r.squared| adj.r.squared|    sigma| statistic|   p.value| df|    logLik|      AIC|      BIC| deviance| df.residual| nobs|\n|---------:|-------------:|--------:|---------:|---------:|--:|---------:|--------:|--------:|--------:|-----------:|----:|\n| 0.8083816|     0.6843933| 4.716024|  6.519819| 0.0003417| 11| -78.38304| 182.7661| 200.5409| 378.0950|          17|   29|\n| 0.6996728|     0.6343843| 5.075926| 10.716628| 0.0000207|  5| -84.89886| 183.7977| 193.3688| 592.5957|          23|   29|\n\n</div>\n:::\n:::\n\n\n\nThese models have R2 values of 0.808 and 0.700, and adjusted R2 values of 0.684 and 0.634. mod1 is preferable to mod2, suggesting we need the interaction term. However, a residual-vs-predicted plot of mod2 has a classic “horn” shape (curving and fanning out), indicating a situation where a response transformation might help better than including the interaction.\n\nAfter trial and error, it turns out that an inverse (reciprocal) transformation, (1/conc) serves really well. (Perhaps this isn’t too surprising, as concentrations are often determined by titration, in which the actual measurements are volumes of some counter-reactant; and these are reciprocally related to concentrations, i.e., amounts per unit volume.) In a real experiment, I would read the experimetnal protocol to verify this idea, or speak with the scientist conducting the experiment.\n\nSo here are three more models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod3 <- update(mod1, inverse(conc) ~ .)\nmod4 <- update(mod2, inverse(conc) ~ .)     # no interaction\nmod5 <- update(mod4, . ~ source + percent)  # continuous (non-factor) term for percent\npar(mfrow = c(2,2)); plot(mod5)\n```\n\n::: {.cell-output-display}\n![](xxxx_xx_xx-emmeans_estimated_marginal_means_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_dfr(list(mod1, mod2, mod3, mod4, mod5), glance, .id = \"model\")\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|model | r.squared| adj.r.squared|     sigma| statistic|   p.value| df|    logLik|       AIC|       BIC|    deviance| df.residual| nobs|\n|:-----|---------:|-------------:|---------:|---------:|---------:|--:|---------:|---------:|---------:|-----------:|-----------:|----:|\n|1     | 0.8083816|     0.6843933| 4.7160240|  6.519819| 0.0003417| 11| -78.38304|  182.7661|  200.5409| 378.0950000|          17|   29|\n|2     | 0.6996728|     0.6343843| 5.0759265| 10.716628| 0.0000207|  5| -84.89886|  183.7977|  193.3688| 592.5956760|          23|   29|\n|3     | 0.8175928|     0.6995647| 0.0031259|  6.927099| 0.0002349| 11| 133.86797| -241.7359| -223.9611|   0.0001661|          17|   29|\n|4     | 0.7866455|     0.7402641| 0.0029065| 16.960361| 0.0000005|  5| 131.59563| -249.1912| -239.6202|   0.0001943|          23|   29|\n|5     | 0.7487292|     0.7185767| 0.0030254| 24.831412| 0.0000001|  3| 129.22377| -248.4475| -241.6111|   0.0002288|          25|   29|\n\n</div>\n:::\n:::\n\n\n\n::: {###reciprocal-fun-model .callout-note}\nI could have used `1/conc` as the response variable, but `emmeans` provides an equivalent `inverse()` function that will prove more advantageous later.\n:::\n\nThe residual plots for these models look a lot more like a random scatter of points (and that is good). The R^2 values for these models are 0.818, 0.787, and 0.749, respectively; and the adjusted R^2s are 0.700, 0.740, and 0.719. mod4 has the best adjusted R^2 and will be our choice.\n\n# Estimated marginal means\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(EMM.source <- emmeans(mod4, specs = \"source\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n source emmean       SE df lower.CL upper.CL\n fish   0.0337 0.000926 23   0.0318   0.0356\n soy    0.0257 0.000945 23   0.0237   0.0276\n skim   0.0229 0.000994 23   0.0208   0.0249\n\nResults are averaged over the levels of: percent \nResults are given on the inverse (not the response) scale. \nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(EMM.percent <- emmeans(mod4, specs = \"percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n percent emmean       SE df lower.CL upper.CL\n       9 0.0322 0.001032 23   0.0301   0.0344\n      12 0.0270 0.000969 23   0.0250   0.0290\n      15 0.0263 0.001104 23   0.0240   0.0286\n      18 0.0241 0.001337 23   0.0213   0.0268\n\nResults are averaged over the levels of: source \nResults are given on the inverse (not the response) scale. \nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\n\nCalling `tidy()` (from `broom`) on the object will put it into a beautiful data frame. And data frames can be plotted.\n\nThe input type can be set to “response”, indicating that values should be back-transformed. Note that the back-transformation is done as the last step, so all tests are conducted on the transformed scaled.\n\nThe package `emmeans` supports `confint` (confidence intervals) and `test` (hypothesis testing) using a function parameter called `type`. The parameter `type` only has an effect if there is a known transformation or link function. The parameter value \"response\" specifies that the inverse transformation be applied, and in that case, the reported values (estimates, standard errors, confidence internals) will be on the original response scale; otherwise, the output is on the non-linear predictor scale, as defined in the formula for the regression.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(confint(EMM.percent, type = \"response\"))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| percent| response| std.error| df| conf.low| conf.high|\n|-------:|--------:|---------:|--:|--------:|---------:|\n|       9| 31.01002| 0.9926247| 23| 29.08415|  33.20903|\n|      12| 37.03236| 1.3286375| 23| 34.47376|  40.00120|\n|      15| 38.05676| 1.5989204| 23| 35.01363|  41.67922|\n|      18| 41.53107| 2.3061140| 23| 37.25203|  46.92072|\n\n</div>\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(confint(EMM.percent, type = \"response\")) %>%\n  ggplot(aes(y = response, x = percent)) +\n  geom_col(width = 1) +\n  geom_errorbar(width = 0.5,\n                aes(ymin = conf.low,\n                    ymax = conf.high))\n```\n\n::: {.cell-output-display}\n![](xxxx_xx_xx-emmeans_estimated_marginal_means_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans::test(EMM.percent, type = \"response\")\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| percent| response|        SE| df| null|  t.ratio| p.value|\n|-------:|--------:|---------:|--:|----:|--------:|-------:|\n|       9| 31.01002| 0.9926247| 23|  Inf| 31.24043|       0|\n|      12| 37.03236| 1.3286375| 23|  Inf| 27.87243|       0|\n|      15| 38.05676| 1.5989204| 23|  Inf| 23.80154|       0|\n|      18| 41.53107| 2.3061140| 23|  Inf| 18.00911|       0|\n\n</div>\n:::\n:::\n\n\n\n## Comparison to ordinary means\nLet’s compare these with the ordinary marginal means (OMMs) on inverse(conc):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(pigs, tapply(inverse(conc), source, mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      fish        soy       skim \n0.03331687 0.02632333 0.02372024 \n```\n\n\n:::\n:::\n\n\n\nThe above code is [in the style of](https://www.rostrum.blog/posts/2024-05-08-aesthetic/) base R [@Dray2024]. Can I write the above ordinary means in Tidyverse/dplyr language?\n\n\n::: {.cell}\n\n```{.r .cell-code}\npigs %>%\n  mutate(conc = inverse(conc)) %>%\n  summarize(mean = mean(conc),\n            \"n\" = n(),\n            .by = source)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|source |      mean|  n|\n|:------|---------:|--:|\n|fish   | 0.0333169| 10|\n|soy    | 0.0263233| 10|\n|skim   | 0.0237202|  9|\n\n</div>\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(pigs, tapply(inverse(conc), percent, mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         9         12         15         18 \n0.03146170 0.02700341 0.02602757 0.02659336 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npigs %>%\n  mutate(conc = inverse(conc)) %>%\n  summarize(mean = mean(conc),\n            \"n\" = n(),\n            .by = percent)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| percent|      mean|  n|\n|-------:|---------:|--:|\n|       9| 0.0314617|  8|\n|      12| 0.0270034|  9|\n|      15| 0.0260276|  7|\n|      18| 0.0265934|  5|\n\n</div>\n:::\n:::\n\n\n\nBoth sets of OMMs are vaguely similar to the corresponding EMMs. However, please note that the EMMs for percent form a decreasing sequence, while the the OMMs decrease but then increase at the end.\n\n# The reference grid, and definition of EMMs\nEstimated marginal means are defined as marginal means of model predictions over the grid comprising all factor combinations – called the reference grid. For the example at hand, the reference grid is\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans::ref_grid(mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'emmGrid' object with variables:\n    source = fish, soy, skim\n    percent =  9, 12, 15, 18\nTransformation: \"inverse\" \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(RG <- expand.grid(source = levels(pigs$source), percent = unique(pigs$percent)))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|source | percent|\n|:------|-------:|\n|fish   |       9|\n|soy    |       9|\n|skim   |       9|\n|fish   |      12|\n|soy    |      12|\n|skim   |      12|\n|fish   |      15|\n|soy    |      15|\n|skim   |      15|\n|fish   |      18|\n|soy    |      18|\n|skim   |      18|\n\n</div>\n:::\n:::\n\n\n\nTo get the EMMs, we first need to obtain predictions on this grid:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(preds <- matrix(predict(mod4, newdata = RG), nrow = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]       [,2]       [,3]       [,4]\n[1,] 0.03853514 0.03329091 0.03256404 0.03036586\n[2,] 0.03050486 0.02526063 0.02453376 0.02233558\n[3,] 0.02770292 0.02245869 0.02173182 0.01953364\n```\n\n\n:::\n:::\n\n\n\nthen obtain the marginal means of these predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\napply(preds, 1, mean)   # row means -- for source\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03368899 0.02565870 0.02285677\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\napply(preds, 2, mean)   # column means -- for percent\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03224764 0.02700341 0.02627654 0.02407836\n```\n\n\n:::\n:::\n\n\n\nThese marginal averages match the EMMs obtained earlier via `emmeans()`.\n\nNow let’s go back to the comparison with the ordinary marginal means. The source levels are represented by the columns of `pred`; and note that each row of `pred` is a decreasing set of values. So it is no wonder that the marginal means – the EMMs for source – are decreasing. That the OMMs for percent do not behave this way is due to the imbalance in sample sizes:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(pigs, table(source, percent))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      percent\nsource 9 12 15 18\n  fish 2  3  2  3\n  soy  3  3  3  1\n  skim 3  3  2  1\n```\n\n\n:::\n:::\n\n\n\nThis shows that the OMMs of the last column give most of the weight (3/5) to the first source, which tends to have higher inverse(conc), making the OMM for 18 percent higher than that for 15 percent, even though the reverse is true with every level of source. This kind of disconnect is an example of Simpson’s paradox, in which a confounding factor can distort your findings. The EMMs are not subject to this paradox, but the OMMs are, when the sample sizes are correlated with the expected values.\n\nIn summary, we obtain a references grid of all factor combinations, obtain model predictions on that grid, and then the expected marginal means are estimated as equally-weighted marginal averages of those predictions. Those EMMs are not subject to confounding by other factors, such as might happen with ordinary marginal means of the data. Moreover, unlike OMMs, EMMs are based on a model that is fitted to the data.\n\n# References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "xxxx_xx_xx-emmeans_estimated_marginal_means_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}